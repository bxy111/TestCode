1.  线性表（Linear List），就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。
    相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。
    
2.  如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。
    在这种情况下，如果要将某个数组插入到第k个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，
    直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。

3.  数组a[10]中存储了8个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。
    为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。
    当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。
    这很像是JVM标记清除垃圾回收算法的核心思想。

4.  基于链表实现 LRU 缓存淘汰算法;
    维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。
    如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。
    如果此数据没有在缓存链表中，又可以分为两种情况：
    如果此时缓存未满，则将此结点直接插入到链表的头部；
    如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

5.  回文字符串;
    使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。这样当快指针指向末尾时，慢指针指向了中点。
    在慢指针前进的过程中，同时修改其 next 指针指向上一个元素prev，使得链表前半部分反序。
    最后比较中点两侧的链表是否相等。

6.  哨兵结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了;
    实际上，这种利用哨兵简化编程难度的技巧，在插入排序、归并排序、动态规划等代码都有用到。

7.  下面 5 个常见的链表操作。只要把这几个操作都能写熟练，不熟就多写几遍，之后再也不会害怕写链表代码。
    单链表反转 (3个指针去做)
    链表中环的检测 (一个快一个慢，如果有环总会相遇)
    两个有序的链表合并
    删除链表倒数第 n 个结点 (第二个指针等第一个指针走了n-1步之后，在移动)
    求链表的中间结点 (一个加2，一个加1)

8.  从栈的操作特性上来看，栈是一种“操作受限”的线性表，只允许在一端插入和删除数据。
    栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。

9.  空间复杂度是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。
    在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。
    不管是顺序栈还是链式栈，入栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是 O(1)。

10. 均摊时间复杂度一般都等于最好情况时间复杂度。因为在大部分情况下，入栈操作的时间复杂度都是 O(1)，只有在个别时刻才会退化为 O(n)，
    所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。

11. 栈，手写了实现四则运算的程序，使用两个栈实现，一个是操作符栈，一个是数字栈;

12. 队列 主要是循环队列;

13. 编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。
    递归代码要警惕重复计算;

14. 我只讲众多排序算法中的一小撮，也是最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

15. 仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。这个概念是说，
    如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

16. 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，是一个原地排序算法。

17. 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，
    我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。
    平均情况下的时间复杂度就是 O(n^2);

18. 首先，我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，
    在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

19. 从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。
    在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。
    平均时间复杂度为 O(n^2);

20. 选择排序空间复杂度为 O(1)，是一种原地排序算法。选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n^2);
    选择排序是一种不稳定的排序算法。

21. 冒泡排序和插入排序的时间复杂度都是 O(n^2)都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？
    虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n^2 );
    但是如果我们希望把性能优化做到极致，那肯定首选插入排序。插入排序的算法思路也有很大的优化空间，我们只是讲了最基础的一种。
    如果你对插入排序的优化感兴趣，可以自行学习一下希尔排序。

22. 冒泡排序、插入排序、选择排序这三种排序算法，它们的时间复杂度都是 O(n2)，比较高，适合小规模数据的排序;
    今天，我讲两种时间复杂度为O(nlogn)的排序算法,归并排序和快速排序这两种排序算法适合大规模的数据排序，比上一节讲的那三种排序算法要更常用。

23. 归并排序算法是一种在任何情况下时间复杂度都比较稳定的排序算法，这也使它存在致命的缺点，
    即归并排序不是原地排序算法，空间复杂度比较高，是 O(n)。正因为此，它也没有快排应用广泛。

24. O(n) 时间复杂度内求无序数组中的第 K 大元素?
    运用快排的思想

25. 今天，我会讲三种时间复杂度是 O(n) 的排序算法：桶排序、计数排序、基数排序。

26. 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。

27. 今天，我们学习了 3 种线性时间复杂度的排序算法，有桶排序、计数排序、基数排序。它们对要排序的数据都有比较苛刻的要求，应用不是非常广泛。
    但是如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。

28. 如何实现一个通用的、高性能的排序函数？
    线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法。
    如果对小规模数据进行排序，可以选择时间复杂度是O(n^2)的算法；如果对大规模数据进行排序，时间复杂度是 O(nlogn)的算法更加高效。
    所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。
    时间复杂度是O(nlogn)的排序算法不止一个，我们已经讲过的有归并排序、快速排序，后面讲堆的时候我们还会讲到堆排序。
    堆排序和快速排序都有比较多的应用，比如Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。

29. 所以，对于小规模数据的排序，O(n^2)的排序算法并不一定比O(nlogn)排序算法执行的时间长。对于小数据量的排序，我们选择比较简单、不需要递归的插入排序算法;

30. 今天我们讲一种针对有序数据集合的查找算法：二分查找（Binary Search）算法，也叫折半查找算法;时间复杂度就是 O(logn);
    那什么情况下适合用二分查找，什么情况下不适合呢？
    首先，二分查找依赖的是顺序表结构，简单点说就是数组。
    那二分查找能否依赖其他数据结构呢？比如链表。答案是不可以的，主要原因是二分查找算法需要按照下标随机访问元素。
    我们在数组和链表那两节讲过，数组按照下标随机访问数据的时间复杂度是 O(1)，而链表随机访问的时间复杂度是 O(n)。
    所以，如果数据使用链表存储，二分查找的时间复杂就会变得很高。
    其次，二分查找针对的是有序数据。

31. 二分查找虽然性能比较优秀，但应用场景也比较有限。底层必须依赖数组，并且还要求数据是有序的。对于较小规模的数据查找，我们直接使用顺序遍历就可以了，
    二分查找的优势并不明显。二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

32. 上两节我们讲了二分查找算法。当时我讲到，因为二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现。如果数据存储在链表中，就真的没法用二分查找算法了吗？
    实际上，我们只需要对链表稍加改造，就可以支持类似“二分”的查找算法。我们把改造之后的数据结构叫作跳表(Skip list),也就是今天要讲的内容。

33. 这种链表加多级索引的结构，就是跳表;
    在跳表中查询任意数据的时间复杂度就是 O(logn)。这个查找的时间复杂度跟二分查找是一样的。换句话说，我们其实是基于单链表实现了二分查找，是不是很神奇？
    不过，天下没有免费的午餐，这种查询效率的提升，前提是建立了很多级索引，也就是我们讲过的空间换时间的设计思路。

34. 今天我们讲了跳表这种数据结构。跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，
    支持快速的插入、删除、查找操作，时间复杂度都是 O(logn)。
    跳表的空间复杂度是 O(n)。不过，跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。虽然跳表的代码实现并不简单，
    但是作为一种动态数据结构，比起红黑树来说，实现要简单多了。所以很多时候，我们为了代码的简单、易读，比起红黑树，我们更倾向用跳表。

35. 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

36. 散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，
    然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

37. 再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类,开放寻址法(open addressing)和链表法(chaining)。
    链表法是一种更加常用的散列冲突解决办法,相比开放寻址法,它要简单很多。我们来看这个图,在散列表中,每个桶(bucket)或者槽(slot)会对应一条链表，
    所有散列值相同的元素我们都放到相同槽位对应的链表中。

38. 实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，
    比如跳表、红黑树。这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。
    这样也就有效避免了前面讲到的散列碰撞攻击。

39. 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。

40. 将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值;
    从哈希值不能反向推导出原始数据(所以哈希算法也叫单向哈希算法);
    对输入数据非常敏感,哪怕原始数据只修改了一个Bit,最后得到的哈希值也大不相同;
    散列冲突的概率要很小,对于不同的原始数据,哈希值相同的概率非常小;
    哈希算法的执行效率要尽量高效,针对较长的文本,也能快速地计算出哈希值.

41. 哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储.

42. 叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。
    叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。

43. 想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。
    前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
    中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
    后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。
    二叉树遍历的时间复杂度是O(n);

44. 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值;
    中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是O(n)，非常高效。

45. 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，
    也就是说，把这个新插入的数据当作大于这个节点的值来处理。
    当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，
    才停止。这样就可以把键值等于要查找值的所有节点都找出来。

46. 散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1),非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是O(logn)，
    相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？
    第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，
            就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
    第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，
            我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。
    第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，
            所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
    第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。
            平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。
    最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。
    综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。

47. 平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于1;
    平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。
    这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。

48. 红黑树的英文是“Red-Black Tree”，简称 R-B Tree。它是一种不严格的平衡二叉查找树，我前面说了，它的定义是不严格符合平衡二叉查找树的定义的;
    顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：
    根节点是黑色的;
    每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据;
    任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的;
    每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点;

49. 我们今天讲另外一种特殊的树，“堆”（HeapHeap）。堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。
    堆排序是一种原地的、时间复杂度为 O(nlogn)O(nlogn) 的排序算法。
    堆是一个完全二叉树;
    堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值;
    对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。











